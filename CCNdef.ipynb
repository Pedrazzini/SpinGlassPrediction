{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T09:34:14.093267Z",
     "start_time": "2024-08-23T09:34:13.944896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OPZIONE VALIDA(? DUE PESI CHE RAPPRESENTANO LO STESSO PESO POTREBBERO ESSERE DIVERSI FRA LORO), MA MOLTO SCOMODA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Rete 1: Un neurone di input connesso a due neuroni di output\n",
    "class SubNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet1, self).__init__()\n",
    "        self.fc = nn.Linear(1, 2, bias=False)  # 1 input, 2 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Rete 2: Un neurone di input connesso a un neurone di output\n",
    "class SubNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet2, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1, bias=False)  # 1 input, 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Rete 3: Un neurone di input connesso a un neurone di output\n",
    "class SubNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet3, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1, bias=False)  # 1 input, 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Inizializza le sotto-reti\n",
    "net1 = SubNet1()\n",
    "net2 = SubNet2()\n",
    "net3 = SubNet3()\n",
    "\n",
    "# Esempio di input\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=False)\n",
    "target = torch.tensor([[0.5, 0.5]], requires_grad=False)\n",
    "\n",
    "# Ottimizzatori per ciascuna sotto-rete\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=0.01)\n",
    "optimizer3 = optim.SGD(net3.parameters(), lr=0.01)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Addestramento\n",
    "for epoch in range(10):\n",
    "    # Azzerare i gradienti\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    optimizer3.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    out1 = net1(x[:, 0:1])  # Usa solo il primo input per la rete 1\n",
    "    out2 = net2(x[:, 1:2])  # Usa solo il secondo input per la rete 2\n",
    "    out3 = net3(x[:, 2:3])  # Usa solo il terzo input per la rete 3\n",
    "\n",
    "    # y_pred1 è la somma dell'output della rete 1 e della rete 2\n",
    "    y_pred1 = out1[:, 0:1] + out2\n",
    "    # y_pred2 è la somma dell'output della rete 1 e della rete 3\n",
    "    y_pred2 = out1[:, 1:2] + out3\n",
    "\n",
    "    # Combina i risultati per ottenere l'output finale\n",
    "    output = torch.cat([y_pred1, y_pred2], dim=1)\n",
    "\n",
    "    # Calcola la loss\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Ottimizzazione\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "\n",
    "    # Stampa i pesi e la loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "    print(\"Pesi SubNet1:\", list(net1.parameters()))\n",
    "    print(\"Pesi SubNet2:\", list(net2.parameters()))\n",
    "    print(\"Pesi SubNet3:\", list(net3.parameters()))\n"
   ],
   "id": "f253aa0f521473c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.048243045806885\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2880],\n",
      "        [-0.1509]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.5428]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.7893]], requires_grad=True)]\n",
      "Epoch 2, Loss: 4.938197135925293\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2793],\n",
      "        [-0.1207]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.5254]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.6987]], requires_grad=True)]\n",
      "Epoch 3, Loss: 4.0352463722229\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2710],\n",
      "        [-0.0935]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.5088]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.6172]], requires_grad=True)]\n",
      "Epoch 4, Loss: 3.3004138469696045\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2631],\n",
      "        [-0.0691]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4930]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.5439]], requires_grad=True)]\n",
      "Epoch 5, Loss: 2.7020926475524902\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2556],\n",
      "        [-0.0471]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4780]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.4779]], requires_grad=True)]\n",
      "Epoch 6, Loss: 2.214648485183716\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2485],\n",
      "        [-0.0272]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4638]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.4184]], requires_grad=True)]\n",
      "Epoch 7, Loss: 1.8172882795333862\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[ 0.2418],\n",
      "        [-0.0094]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4503]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.3650]], requires_grad=True)]\n",
      "Epoch 8, Loss: 1.4931429624557495\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[0.2353],\n",
      "        [0.0066]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4374]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.3168]], requires_grad=True)]\n",
      "Epoch 9, Loss: 1.228524088859558\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[0.2292],\n",
      "        [0.0211]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4252]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.2735]], requires_grad=True)]\n",
      "Epoch 10, Loss: 1.0123226642608643\n",
      "Pesi SubNet1: [Parameter containing:\n",
      "tensor([[0.2234],\n",
      "        [0.0341]], requires_grad=True)]\n",
      "Pesi SubNet2: [Parameter containing:\n",
      "tensor([[0.4136]], requires_grad=True)]\n",
      "Pesi SubNet3: [Parameter containing:\n",
      "tensor([[-0.2345]], requires_grad=True)]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:53:30.094135Z",
     "start_time": "2024-08-24T09:53:30.064883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# METODO MIGLIORE CHE ESISTA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definizione del modello\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(3, 2, bias=False)  # 3 input, 2 output, no bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Funzione di aggiornamento manuale\n",
    "def manual_update(model, learning_rate, mask=None):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                if mask is not None:\n",
    "                    param.grad *= mask\n",
    "                param -= learning_rate * param.grad\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = SimpleNet()\n",
    "\n",
    "# Definizione della funzione di perdita\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Creazione della maschera (ad esempio, per congelare il primo e il terzo peso della prima colonna)\n",
    "mask = torch.tensor([[0, 1, 0],  # Maschera per il layer 0\n",
    "                     [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Definizione del learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Dati di esempio per l'addestramento\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0],\n",
    "                       [7.0, 8.0, 9.0],\n",
    "                       [10.0, 11.0, 12.0]])\n",
    "\n",
    "targets = torch.tensor([[0.5, 0.5],\n",
    "                        [1.0, 1.0],\n",
    "                        [1.5, 1.5],\n",
    "                        [2.0, 2.0]])\n",
    "\n",
    "# Numero di epoche di addestramento\n",
    "num_epochs = 10\n",
    "\n",
    "# Processo di addestramento\n",
    "for epoch in range(num_epochs):\n",
    "    # Azzerare i gradienti\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Passaggio avanti\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Calcolare la loss\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Passaggio indietro (calcolare i gradienti)\n",
    "    loss.backward()\n",
    "\n",
    "    # Aggiornare i pesi manualmente\n",
    "    manual_update(model, learning_rate, mask)\n",
    "\n",
    "    # Stampa della loss e dei pesi ogni epoca\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"Pesi della rete:\", list(model.parameters()))\n"
   ],
   "id": "8465cfaec0e6f93d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 8.1044\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.1335, -0.4391],\n",
      "        [-0.3598,  0.4738, -0.0783]], requires_grad=True)]\n",
      "Epoch [2/10], Loss: 1.8310\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.2543, -0.4391],\n",
      "        [-0.3598,  0.5364, -0.0783]], requires_grad=True)]\n",
      "Epoch [3/10], Loss: 0.4745\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3105, -0.4391],\n",
      "        [-0.3598,  0.5655, -0.0783]], requires_grad=True)]\n",
      "Epoch [4/10], Loss: 0.1812\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3367, -0.4391],\n",
      "        [-0.3598,  0.5790, -0.0783]], requires_grad=True)]\n",
      "Epoch [5/10], Loss: 0.1178\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3488, -0.4391],\n",
      "        [-0.3598,  0.5853, -0.0783]], requires_grad=True)]\n",
      "Epoch [6/10], Loss: 0.1041\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3545, -0.4391],\n",
      "        [-0.3598,  0.5882, -0.0783]], requires_grad=True)]\n",
      "Epoch [7/10], Loss: 0.1011\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3571, -0.4391],\n",
      "        [-0.3598,  0.5896, -0.0783]], requires_grad=True)]\n",
      "Epoch [8/10], Loss: 0.1005\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3583, -0.4391],\n",
      "        [-0.3598,  0.5902, -0.0783]], requires_grad=True)]\n",
      "Epoch [9/10], Loss: 0.1003\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3589, -0.4391],\n",
      "        [-0.3598,  0.5905, -0.0783]], requires_grad=True)]\n",
      "Epoch [10/10], Loss: 0.1003\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.3642,  0.3591, -0.4391],\n",
      "        [-0.3598,  0.5906, -0.0783]], requires_grad=True)]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-24T09:51:48.565861Z",
     "start_time": "2024-08-24T09:51:48.547589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ],
   "id": "8547be9c101e857d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.0147, -0.0000],\n",
      "        [ 0.0000,  0.0430,  0.0000]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T13:09:52.181826Z",
     "start_time": "2024-08-23T13:09:52.140031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ESPERIMENTO 626\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definizione del modello\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(3, 2, bias=False)  # 3 input, 2 output, no bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Funzione di perdita personalizzata (mean squared error)\n",
    "def custom_loss(output, target):\n",
    "    return torch.mean((output - target) ** 2)\n",
    "\n",
    "# Funzione per calcolare manualmente i gradienti e applicare la maschera\n",
    "def manual_backward(model, criterion, inputs, targets, mask):\n",
    "    # Passaggio avanti\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calcolare la perdita\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Calcolare la perdita rispetto agli output\n",
    "    loss_grad = 2 * (outputs - targets) / outputs.size(0)\n",
    "    \n",
    "    # Ottieni i pesi del modello\n",
    "    weights = model.fc.weight\n",
    "    \n",
    "    \n",
    "    # Calcolare la derivata della loss rispetto ai pesi manualmente\n",
    "    weight_derivatives = torch.zeros_like(weights) #Crea un tensore di gradienti della stessa forma dei pesi, inizialmente riempito di zeri\n",
    "    for i in range(weights.size(0)):  # per ogni neurone di output (numero di righe nella matrice W)\n",
    "        for j in range(weights.size(1)):  # per ogni neurone di input\n",
    "            if mask[i][j] != 0:\n",
    "                weight_derivatives[i, j] =  (2 / inputs.size(0)) * torch.sum(inputs[:, j] * loss_grad[:, i])\n",
    "            else:\n",
    "                weight_derivatives[i, j] = 0\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Applicare la maschera ai gradienti\n",
    "    weight_derivatives *= mask\n",
    "    \n",
    "    return loss, weight_derivatives\n",
    "\n",
    "# Funzione di aggiornamento manuale dei pesi\n",
    "def manual_update(model, gradients, learning_rate):\n",
    "    with torch.no_grad():\n",
    "        model.fc.weight -= learning_rate * gradients\n",
    "        model.fc.weight *= mask  # Imposta a 0 i pesi non aggiornati\n",
    "        \n",
    "# Inizializzazione del modello\n",
    "model = SimpleNet()\n",
    "\n",
    "# Definizione della funzione di perdita\n",
    "criterion = custom_loss\n",
    "\n",
    "# Creazione della maschera (ad esempio, per congelare il primo e il terzo peso della prima colonna)\n",
    "mask = torch.tensor([[0, 1, 0],  # Maschera per il layer 0\n",
    "                     [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Definizione del learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Dati di esempio per l'addestramento\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0],\n",
    "                       [7.0, 8.0, 9.0],\n",
    "                       [10.0, 11.0, 12.0]])\n",
    "\n",
    "targets = torch.tensor([[0.5, 0.5],\n",
    "                        [1.0, 1.0],\n",
    "                        [1.5, 1.5],\n",
    "                        [2.0, 2.0]])\n",
    "\n",
    "# Numero di epoche di addestramento\n",
    "num_epochs = 10\n",
    "\n",
    "# Processo di addestramento\n",
    "for epoch in range(num_epochs):\n",
    "    # Calcolare la loss e i gradienti manualmente\n",
    "    loss, gradients = manual_backward(model, criterion, inputs, targets, mask)\n",
    "    \n",
    "    # Aggiornare i pesi manualmente\n",
    "    manual_update(model, gradients, learning_rate)\n",
    "    \n",
    "    # Stampa della loss e dei pesi ogni epoca\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"Pesi della rete:\", list(model.parameters()))\n",
    "\n",
    "# ESPERIMENTO RIUSCITO"
   ],
   "id": "b543baec43449066",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 33.2715\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[ 0.0000, -0.0497, -0.0000],\n",
      "        [-0.0000,  0.0313, -0.0000]], requires_grad=True)]\n",
      "Epoch [2/10], Loss: 2.1518\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.0769, -0.0000],\n",
      "        [-0.0000, 0.1146, -0.0000]], requires_grad=True)]\n",
      "Epoch [3/10], Loss: 0.4699\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1357, -0.0000],\n",
      "        [-0.0000, 0.1533, -0.0000]], requires_grad=True)]\n",
      "Epoch [4/10], Loss: 0.1062\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1631, -0.0000],\n",
      "        [-0.0000, 0.1713, -0.0000]], requires_grad=True)]\n",
      "Epoch [5/10], Loss: 0.0275\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1759, -0.0000],\n",
      "        [-0.0000, 0.1796, -0.0000]], requires_grad=True)]\n",
      "Epoch [6/10], Loss: 0.0105\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1818, -0.0000],\n",
      "        [-0.0000, 0.1835, -0.0000]], requires_grad=True)]\n",
      "Epoch [7/10], Loss: 0.0069\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1845, -0.0000],\n",
      "        [-0.0000, 0.1853, -0.0000]], requires_grad=True)]\n",
      "Epoch [8/10], Loss: 0.0061\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1858, -0.0000],\n",
      "        [-0.0000, 0.1862, -0.0000]], requires_grad=True)]\n",
      "Epoch [9/10], Loss: 0.0059\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1864, -0.0000],\n",
      "        [-0.0000, 0.1866, -0.0000]], requires_grad=True)]\n",
      "Epoch [10/10], Loss: 0.0059\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[0.0000, 0.1867, -0.0000],\n",
      "        [-0.0000, 0.1868, -0.0000]], requires_grad=True)]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:44:33.341675Z",
     "start_time": "2024-08-23T16:44:33.289265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AGGIUNTA DEL BIAS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definizione del modello\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(3, 2, bias=True)  # 3 input, 2 output, con bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Funzione di perdita personalizzata (mean squared error)\n",
    "def custom_loss(output, target):\n",
    "    return torch.mean((output - target) ** 2)\n",
    "\n",
    "# Funzione per calcolare manualmente i gradienti e applicare la maschera\n",
    "def manual_backward(model, criterion, inputs, targets, mask):\n",
    "    # Passaggio avanti\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calcolare la perdita\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Calcolare la perdita rispetto agli output\n",
    "    loss_grad = 2 * (outputs - targets) / outputs.size(0)\n",
    "    \n",
    "    # Ottieni i pesi e il bias del modello\n",
    "    weights = model.fc.weight\n",
    "    biases = model.fc.bias\n",
    "    \n",
    "    # Calcolare la derivata della loss rispetto ai pesi manualmente\n",
    "    weight_derivatives = torch.zeros_like(weights)  # Crea un tensore di gradienti della stessa forma dei pesi, inizialmente riempito di zeri\n",
    "    bias_derivatives = torch.zeros_like(biases)  # Gradienti per il bias\n",
    "    \n",
    "    for i in range(weights.size(0)):  # per ogni neurone di output (numero di righe nella matrice W)\n",
    "        for j in range(weights.size(1)):  # per ogni neurone di input\n",
    "            if mask[i][j] != 0:\n",
    "                weight_derivatives[i, j] = (2 / inputs.size(0)) * torch.sum(inputs[:, j] * loss_grad[:, i])\n",
    "            else:\n",
    "                weight_derivatives[i, j] = 0\n",
    "        \n",
    "        # Calcola il gradiente rispetto al bias per ogni neurone di output\n",
    "        bias_derivatives[i] = (2 / inputs.size(0)) * torch.sum(loss_grad[:, i])\n",
    "    \n",
    "    # Applicare la maschera ai gradienti dei pesi\n",
    "    weight_derivatives *= mask\n",
    "    \n",
    "    return loss, weight_derivatives, bias_derivatives\n",
    "\n",
    "# Funzione di aggiornamento manuale dei pesi e del bias\n",
    "def manual_update(model, weight_gradients, bias_gradients, learning_rate):\n",
    "    with torch.no_grad():\n",
    "        model.fc.weight -= learning_rate * weight_gradients\n",
    "        model.fc.bias -= learning_rate * bias_gradients\n",
    "        model.fc.weight *= mask  # Imposta a 0 i pesi non aggiornati (secondo la maschera)\n",
    "        \n",
    "# Inizializzazione del modello\n",
    "model = SimpleNet()\n",
    "\n",
    "# Definizione della funzione di perdita\n",
    "criterion = custom_loss\n",
    "\n",
    "# Creazione della maschera (ad esempio, per congelare il primo e il terzo peso della prima colonna)\n",
    "mask = torch.tensor([[0, 1, 0],  # Maschera per il layer 0\n",
    "                     [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Definizione del learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Dati di esempio per l'addestramento\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0],\n",
    "                       [7.0, 8.0, 9.0],\n",
    "                       [10.0, 11.0, 12.0]])\n",
    "\n",
    "targets = torch.tensor([[0.5, 0.5],\n",
    "                        [1.0, 1.0],\n",
    "                        [1.5, 1.5],\n",
    "                        [2.0, 2.0]])\n",
    "\n",
    "# Numero di epoche di addestramento\n",
    "num_epochs = 10\n",
    "\n",
    "# Processo di addestramento\n",
    "for epoch in range(num_epochs):\n",
    "    # Calcolare la loss e i gradienti manualmente\n",
    "    loss, weight_gradients, bias_gradients = manual_backward(model, criterion, inputs, targets, mask)\n",
    "    \n",
    "    # Aggiornare i pesi e il bias manualmente\n",
    "    manual_update(model, weight_gradients, bias_gradients, learning_rate)\n",
    "    \n",
    "    # Stampa della loss e dei pesi ogni epoca\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"Pesi della rete:\", list(model.parameters()))\n"
   ],
   "id": "400c045a4918b47b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 26.8107\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.2608, 0.0000],\n",
      "        [0.0000, 0.0042, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0293, -0.3915], requires_grad=True)]\n",
      "Epoch [2/10], Loss: 1.6143\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.2194, 0.0000],\n",
      "        [0.0000, 0.1274, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0245, -0.3753], requires_grad=True)]\n",
      "Epoch [3/10], Loss: 0.3623\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.2004, 0.0000],\n",
      "        [0.0000, 0.1836, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0225, -0.3674], requires_grad=True)]\n",
      "Epoch [4/10], Loss: 0.1007\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1917, 0.0000],\n",
      "        [0.0000, 0.2093, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0217, -0.3631], requires_grad=True)]\n",
      "Epoch [5/10], Loss: 0.0459\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1877, 0.0000],\n",
      "        [0.0000, 0.2209, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0216, -0.3606], requires_grad=True)]\n",
      "Epoch [6/10], Loss: 0.0343\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1859, 0.0000],\n",
      "        [0.0000, 0.2262, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0217, -0.3589], requires_grad=True)]\n",
      "Epoch [7/10], Loss: 0.0318\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1850, 0.0000],\n",
      "        [0.0000, 0.2285, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0219, -0.3575], requires_grad=True)]\n",
      "Epoch [8/10], Loss: 0.0312\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1846, 0.0000],\n",
      "        [0.0000, 0.2295, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0221, -0.3562], requires_grad=True)]\n",
      "Epoch [9/10], Loss: 0.0310\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1844, 0.0000],\n",
      "        [0.0000, 0.2299, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0224, -0.3551], requires_grad=True)]\n",
      "Epoch [10/10], Loss: 0.0308\n",
      "Pesi della rete: [Parameter containing:\n",
      "tensor([[-0.0000, 0.1843, 0.0000],\n",
      "        [0.0000, 0.2300, -0.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0227, -0.3540], requires_grad=True)]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T16:44:35.878568Z",
     "start_time": "2024-08-23T16:44:35.858076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs) # come vedi poi dentro torch le matrici si traspongono e fanno cose strane"
   ],
   "id": "ebcbc634c688fcbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3913, 0.1060],\n",
      "        [0.9442, 0.7959],\n",
      "        [1.4970, 1.4858],\n",
      "        [2.0499, 2.1757]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T17:37:46.486130Z",
     "start_time": "2024-08-23T17:37:46.413222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PER ORA ER MEJO CHE SI PUO'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Definizione del modello con hidden layers e bias\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 4, bias=True)  # 3 input, 4 output (hidden layer) con bias\n",
    "        self.fc2 = nn.Linear(4, 2, bias=True)  # 4 input (hidden layer), 2 output con bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Attivazione ReLU dopo il primo layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Funzione di perdita personalizzata (mean squared error)\n",
    "def custom_loss(output, target):\n",
    "    return torch.mean((output - target) ** 2)\n",
    "\n",
    "# Funzione per calcolare manualmente i gradienti e applicare le maschere\n",
    "def manual_backward(model, criterion, inputs, targets, masks):\n",
    "    # Passaggio avanti\n",
    "    outputs = model(inputs) # outputs è una matrice di dimensione (batch x outputs)\n",
    "    \n",
    "    # Calcolare la perdita\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Calcolare la perdita rispetto agli output\n",
    "    loss_grad = 2 * (outputs - targets) / outputs.size(0)\n",
    "\n",
    "    # Calcolare i gradienti per fc2 (secondo layer) (parto da W2)\n",
    "    weight_derivatives2 = torch.zeros_like(model.fc2.weight)\n",
    "    bias_derivatives2 = torch.zeros_like(model.fc2.bias)  # Gradienti per il bias del secondo layer\n",
    "    hidden_outputs = torch.relu(model.fc1(inputs))\n",
    "    \n",
    "    for i in range(weight_derivatives2.size(0)):  # per ogni neurone di output\n",
    "        for j in range(weight_derivatives2.size(1)):  # per ogni neurone di input (hidden layer)\n",
    "            if masks[1][i][j] != 0:\n",
    "                weight_derivatives2[i, j] = (2 / inputs.size(0)) * torch.sum(hidden_outputs[:, j] * loss_grad[:, i])\n",
    "            else:\n",
    "                weight_derivatives2[i, j] = 0\n",
    "        # Calcolare i gradienti per il bias\n",
    "        bias_derivatives2[i] = (2 / inputs.size(0)) * torch.sum(loss_grad[:, i])\n",
    "\n",
    "    # Calcolare la perdita rispetto al hidden layer\n",
    "    hidden_loss_grad = torch.matmul(loss_grad, model.fc2.weight) * (hidden_outputs > 0).float()  # ReLU derivative### PRIMA DELLA TUA MODIFICA AVEVI model.fc2.weight().t TRASPOSTA\n",
    "\n",
    "    # Calcolare i gradienti per fc1 (primo layer)\n",
    "    weight_derivatives1 = torch.zeros_like(model.fc1.weight)\n",
    "    bias_derivatives1 = torch.zeros_like(model.fc1.bias)  # Gradienti per il bias del primo layer\n",
    "    \n",
    "    for i in range(weight_derivatives1.size(0)):  # per ogni neurone di hidden layer\n",
    "        for j in range(weight_derivatives1.size(1)):  # per ogni neurone di input\n",
    "            if masks[0][i][j] != 0:\n",
    "                weight_derivatives1[i, j] = (2 / inputs.size(0)) * torch.sum(inputs[:, j] * hidden_loss_grad[:, i])\n",
    "            else:\n",
    "                weight_derivatives1[i, j] = 0\n",
    "        # Calcolare i gradienti per il bias\n",
    "        bias_derivatives1[i] = (2 / inputs.size(0)) * torch.sum(hidden_loss_grad[:, i])\n",
    "\n",
    "    # Applicare le maschere ai gradienti\n",
    "    weight_derivatives1 *= masks[0]\n",
    "    weight_derivatives2 *= masks[1]\n",
    "    \n",
    "    return loss, [weight_derivatives1, weight_derivatives2], [bias_derivatives1, bias_derivatives2]\n",
    "\n",
    "# Funzione di aggiornamento manuale dei pesi e dei bias per tutti i layer\n",
    "def manual_update(model, weight_gradients, bias_gradients, learning_rate, masks):\n",
    "    with torch.no_grad():\n",
    "        # Aggiornamento pesi e bias del primo layer\n",
    "        model.fc1.weight -= learning_rate * weight_gradients[0]\n",
    "        model.fc1.bias -= learning_rate * bias_gradients[0]\n",
    "        model.fc1.weight *= masks[0]  # Imposta a 0 i pesi non aggiornati del primo layer\n",
    "        \n",
    "        # Aggiornamento pesi e bias del secondo layer\n",
    "        model.fc2.weight -= learning_rate * weight_gradients[1]\n",
    "        model.fc2.bias -= learning_rate * bias_gradients[1]\n",
    "        model.fc2.weight *= masks[1]  # Imposta a 0 i pesi non aggiornati del secondo layer\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = SimpleNet()\n",
    "\n",
    "# Definizione della funzione di perdita\n",
    "criterion = custom_loss\n",
    "\n",
    "# Creazione delle maschere per ogni layer\n",
    "mask1 = torch.tensor([[0, 1, 0], [1, 0, 1], [0, 1, 1], [1, 0, 0]], dtype=torch.float32)  # Maschera per il primo layer\n",
    "mask2 = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 1]], dtype=torch.float32)  # Maschera per il secondo layer\n",
    "masks = [mask1, mask2]\n",
    "\n",
    "# Definizione del learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Dati di esempio per l'addestramento\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0],\n",
    "                       [7.0, 8.0, 9.0],\n",
    "                       [10.0, 11.0, 12.0],\n",
    "                       [13.0, 14.0, 15.0]])\n",
    "\n",
    "targets = torch.tensor([[0.5, 1.5],\n",
    "                        [1.0, 7.0],\n",
    "                        [6.5, 1.5],\n",
    "                        [2.0, 13.0],\n",
    "                        [3.0, 4.0]])\n",
    "\n",
    "# Numero di epoche di addestramento\n",
    "num_epochs = 10\n",
    "\n",
    "# Processo di addestramento\n",
    "for epoch in range(num_epochs):\n",
    "    # Calcolare la loss e i gradienti manualmente\n",
    "    loss, gradients, bias_gradients = manual_backward(model, criterion, inputs, targets, masks)\n",
    "    \n",
    "    # Aggiornare i pesi e i bias manualmente\n",
    "    manual_update(model, gradients, bias_gradients, learning_rate, masks)\n",
    "    \n",
    "    # Stampa della loss e dei pesi ogni epoca\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        #print(\"Pesi del primo layer:\", model.fc1.weight)\n",
    "        #print(\"Bias del primo layer:\", model.fc1.bias)\n",
    "        #rint(\"Pesi del secondo layer:\", model.fc2.weight)\n",
    "        #print(\"Bias del secondo layer:\", model.fc2.bias)\n"
   ],
   "id": "342a209429329047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 37.3030\n",
      "Epoch [2/10], Loss: 25.9641\n",
      "Epoch [3/10], Loss: 23.7486\n",
      "Epoch [4/10], Loss: 21.8733\n",
      "Epoch [5/10], Loss: 20.2861\n",
      "Epoch [6/10], Loss: 18.9427\n",
      "Epoch [7/10], Loss: 17.8057\n",
      "Epoch [8/10], Loss: 16.8433\n",
      "Epoch [9/10], Loss: 16.0287\n",
      "Epoch [10/10], Loss: 15.3392\n"
     ]
    }
   ],
   "execution_count": 67
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
